import json
import pickle
import struct
from pathlib import Path

import numpy as np
import torch
from tqdm import tqdm


class MyDataset(torch.utils.data.Dataset):
    """
    ç”¨æˆ·åºåˆ—æ•°æ®é›†

    Args:
        data_dir: æ•°æ®æ–‡ä»¶ç›®å½•
        args: å…¨å±€å‚æ•°

    Attributes:
        data_dir: æ•°æ®æ–‡ä»¶ç›®å½•
        maxlen: æœ€å¤§é•¿åº¦
        item_feat_dict: ç‰©å“ç‰¹å¾å­—å…¸
        mm_emb_ids: æ¿€æ´»çš„mm_embç‰¹å¾ID
        mm_emb_dict: å¤šæ¨¡æ€ç‰¹å¾å­—å…¸
        itemnum: ç‰©å“æ•°é‡
        usernum: ç”¨æˆ·æ•°é‡
        indexer_i_rev: ç‰©å“ç´¢å¼•å­—å…¸ (reid -> item_id)
        indexer_u_rev: ç”¨æˆ·ç´¢å¼•å­—å…¸ (reid -> user_id)
        indexer: ç´¢å¼•å­—å…¸
        feature_default_value: ç‰¹å¾ç¼ºçœå€¼
        feature_types: ç‰¹å¾ç±»å‹ï¼Œåˆ†ä¸ºuserå’Œitemçš„sparse, array, emb, continualç±»å‹
        feat_statistics: ç‰¹å¾ç»Ÿè®¡ä¿¡æ¯ï¼ŒåŒ…æ‹¬userå’Œitemçš„ç‰¹å¾æ•°é‡
    """

    def __init__(self, data_dir, args):
        """
        åˆå§‹åŒ–æ•°æ®é›†
        """
        super().__init__()
        self.data_dir = Path(data_dir)
        self._load_data_and_offsets()
        self.maxlen = args.maxlen
        self.mm_emb_ids = args.mm_emb_id

        self.item_feat_dict = json.load(open(Path(data_dir, "item_feat_dict.json"), 'r'))  # key: item_reid, value: {feture_reid, value}
        self.mm_emb_dict = load_mm_emb(Path(data_dir, "creative_emb"), self.mm_emb_ids)
        with open(self.data_dir / 'indexer.pkl', 'rb') as ff:
            indexer = pickle.load(ff)
            self.itemnum = len(indexer['i'])
            self.usernum = len(indexer['u'])
        self.indexer_i_rev = {v: k for k, v in indexer['i'].items()}  # è¿™é‡Œä¸»è¦ç”¨äº† åŸå§‹item id-> reid çš„æ˜ å°„ï¼Œå› ä¸ºç‰©å“çš„mmembçš„idæ˜¯åŸå§‹item id
        self.indexer_u_rev = {v: k for k, v in indexer['u'].items()}
        self.indexer = indexer

        self.feature_default_value, self.feature_types, self.feat_statistics = self._init_feat_info()

    def _load_data_and_offsets(self):
        """
        åŠ è½½ç”¨æˆ·åºåˆ—æ•°æ®å’Œæ¯ä¸€è¡Œçš„æ–‡ä»¶åç§»é‡(é¢„å¤„ç†å¥½çš„), ç”¨äºå¿«é€Ÿéšæœºè®¿é—®æ•°æ®å¹¶I/O
        """
        self.data_file = open(self.data_dir / "seq.jsonl", 'rb')
        with open(Path(self.data_dir, 'seq_offsets.pkl'), 'rb') as f:
            self.seq_offsets = pickle.load(f)

    def _load_user_data(self, uid):
        """
        ä»æ•°æ®æ–‡ä»¶ä¸­åŠ è½½å•ä¸ªç”¨æˆ·çš„æ•°æ®

        Args:
            uid: ç”¨æˆ·ID(reid)

        Returns:
            data: ç”¨æˆ·åºåˆ—æ•°æ®ï¼Œæ ¼å¼ä¸º[(user_id, item_id, user_feat, item_feat, action_type, timestamp)]
        """
        self.data_file.seek(self.seq_offsets[uid])
        line = self.data_file.readline()
        data = json.loads(line)
        return data

    def _random_neq(self, l, r, s):
        """
        ç”Ÿæˆä¸€ä¸ªä¸åœ¨åºåˆ—sä¸­çš„éšæœºæ•´æ•°, ç”¨äºè®­ç»ƒæ—¶çš„è´Ÿé‡‡æ ·

        Args:
            l: éšæœºæ•´æ•°çš„æœ€å°å€¼
            r: éšæœºæ•´æ•°çš„æœ€å¤§å€¼
            s: åºåˆ—

        Returns:
            t: ä¸åœ¨åºåˆ—sä¸­çš„éšæœºæ•´æ•°
        """
        t = np.random.randint(l, r)
        while t in s or str(t) not in self.item_feat_dict:
            t = np.random.randint(l, r)
        return t

    def __getitem__(self, uid):
        """
        è·å–å•ä¸ªç”¨æˆ·çš„æ•°æ®ï¼Œå¹¶è¿›è¡Œpaddingå¤„ç†ï¼Œç”Ÿæˆæ¨¡å‹éœ€è¦çš„æ•°æ®æ ¼å¼

        Args:
            uid: ç”¨æˆ·ID(reid)

        Returns:
            seq: ç”¨æˆ·åºåˆ—ID
            pos: æ­£æ ·æœ¬IDï¼ˆå³ä¸‹ä¸€ä¸ªçœŸå®è®¿é—®çš„itemï¼‰
            neg: è´Ÿæ ·æœ¬ID
            token_type: ç”¨æˆ·åºåˆ—ç±»å‹ï¼Œ1è¡¨ç¤ºitemï¼Œ2è¡¨ç¤ºuser
            next_token_type: ä¸‹ä¸€ä¸ªtokenç±»å‹ï¼Œ1è¡¨ç¤ºitemï¼Œ2è¡¨ç¤ºuser
            seq_feat: ç”¨æˆ·åºåˆ—ç‰¹å¾ï¼Œæ¯ä¸ªå…ƒç´ ä¸ºå­—å…¸ï¼Œkeyä¸ºç‰¹å¾IDï¼Œvalueä¸ºç‰¹å¾å€¼
            pos_feat: æ­£æ ·æœ¬ç‰¹å¾ï¼Œæ¯ä¸ªå…ƒç´ ä¸ºå­—å…¸ï¼Œkeyä¸ºç‰¹å¾IDï¼Œvalueä¸ºç‰¹å¾å€¼
            neg_feat: è´Ÿæ ·æœ¬ç‰¹å¾ï¼Œæ¯ä¸ªå…ƒç´ ä¸ºå­—å…¸ï¼Œkeyä¸ºç‰¹å¾IDï¼Œvalueä¸ºç‰¹å¾å€¼
        """
        user_sequence = self._load_user_data(uid)  # åŠ¨æ€åŠ è½½ç”¨æˆ·æ•°æ®

        ext_user_sequence = []
        for __, record_tuple in enumerate(user_sequence):
            u, i, user_feat, item_feat, action_type, _ = record_tuple
            if u and user_feat:
                ext_user_sequence.insert(0, (u, user_feat, 2, action_type))  # user profile, æ¯ä¸ªuser_seqåªæœ‰ä¸€ä¸ªï¼Œä¸”ä¸€å®šæ˜¯æœ€åä¸€ä¸ªï¼Œæ‰€ä»¥user_profileä¼šè¢«insert(0)åŠ åˆ°ext_user_sequenceçš„æœ€å‰é¢
                u_index = __
            if i and item_feat:
                ext_user_sequence.append((i, item_feat, 1, action_type))  # item interaction
        # print("len(ext_user_sequence):", len(ext_user_sequence), "u_index", u_index)

        # max_lenå–çš„æ˜¯101
        seq = np.zeros([self.maxlen + 1], dtype=np.int32)  # (102, 1)
        pos = np.zeros([self.maxlen + 1], dtype=np.int32)  # (102, 1)
        neg = np.zeros([self.maxlen + 1], dtype=np.int32)  # (102, 1)
        token_type = np.zeros([self.maxlen + 1], dtype=np.int32)  # (102, 1)
        next_token_type = np.zeros([self.maxlen + 1], dtype=np.int32)  # (102, 1)
        next_action_type = np.zeros([self.maxlen + 1], dtype=np.int32)  # (102, 1)

        seq_feat = np.empty([self.maxlen + 1], dtype=object)
        pos_feat = np.empty([self.maxlen + 1], dtype=object)
        neg_feat = np.empty([self.maxlen + 1], dtype=object)

        # nxtæ˜¯è¡Œä¸ºåºåˆ—ä¸­çš„æœ€åä¸€ä¸ªï¼ŒæŒ‰ç…§ç›®å‰çš„ä¹Ÿæ˜¯item interaction, user profileæ˜¯è¡Œä¸ºåºåˆ—çš„ç¬¬ä¸€ä¸ª
        nxt = ext_user_sequence[-1]
        idx = self.maxlen

        ts = set()
        # tsæ˜¯item interactionçš„item idçš„é›†åˆ
        for record_tuple in ext_user_sequence:
            # æ’é™¤user profile
            if record_tuple[2] == 1 and record_tuple[0]:
                ts.add(record_tuple[0])

        # left-padding, ä»åå¾€å‰éå†ï¼Œå°†ç”¨æˆ·åºåˆ—å¡«å……åˆ°maxlen+1çš„é•¿åº¦
        for record_tuple in reversed(ext_user_sequence[:-1]):
            # i: int, item_id
            # feat: dict, å½¢å¦‚ {'112':14,...}
            # type_: intï¼ŒæŒ‰ç…§ä¸Šé¢çš„ï¼Œåªè¦æ˜¯item interaction, éƒ½æ˜¯1
            # act_type: int, 0è¾ƒå¤š,1è¾ƒå°‘,å°‘é‡None
            i, feat, type_, act_type = record_tuple
            next_i, next_feat, next_type, next_act_type = nxt
            feat = self.fill_missing_feat(feat, i)  # dict, å½¢å¦‚ {'112':22, ...}
            next_feat = self.fill_missing_feat(next_feat, next_i)  # dict
            seq[idx] = i  # item_id
            token_type[idx] = type_  # item_interaction, 1
            next_token_type[idx] = next_type  # item_interaction, 1
            # next_action_type: 0, 1, æˆ– None
            if next_act_type is not None:
                next_action_type[idx] = next_act_type
            seq_feat[idx] = feat  # æ›´æ–°featåºåˆ—
            if next_type == 1 and next_i != 0:
                pos[idx] = next_i  # æ­£æ ·æœ¬item idç½®ä¸ºnext_i
                pos_feat[idx] = next_feat  # æ­£æ ·æœ¬featç½®ä¸ºnext_feat
                neg_id = self._random_neq(1, self.itemnum + 1, ts)  # è´Ÿæ ·æœ¬éšæœºé‡‡æ ·ï¼Œä¸ä»tsé‡Œé¢å–
                neg[idx] = neg_id  # è´Ÿæ ·æœ¬item id
                neg_feat[idx] = self.fill_missing_feat(self.item_feat_dict[str(neg_id)], neg_id)  # è´Ÿæ ·æœ¬feat
            nxt = record_tuple  # æ›´æ–°nxtä¸ºå½“å‰record_tuple
            idx -= 1
            # idxåˆå§‹å€¼ä¸º101, -1åå˜æˆ100 åˆšå¼€å§‹æœ€å¤šæ˜¯æ‹¿ 1 user_profile + 99 item_interaction é¢„æµ‹ç¬¬ 100 ä¸ªitem_interaction
            # å°±ç®—æœ€é•¿ï¼Œæœ€åä¸€ä¸ªæ˜¯æ‹¿ 1 user_profile é¢„æµ‹ ç¬¬ 1 ä¸ªitem_interaction 
            # æ‰€ä»¥ä¸€èˆ¬èµ°ä¸åˆ°idx==-1ï¼Œè¿™æ˜¯æ˜¯ä¸ºäº†ä¿é™©èµ·è§
            if idx == -1:
                break

        seq_feat = np.where(seq_feat == None, self.feature_default_value, seq_feat)
        pos_feat = np.where(pos_feat == None, self.feature_default_value, pos_feat)
        neg_feat = np.where(neg_feat == None, self.feature_default_value, neg_feat)

        return seq, pos, neg, token_type, next_token_type, next_action_type, seq_feat, pos_feat, neg_feat

    def __len__(self):
        """
        è¿”å›æ•°æ®é›†é•¿åº¦ï¼Œå³ç”¨æˆ·æ•°é‡

        Returns:
            usernum: ç”¨æˆ·æ•°é‡
        """
        return len(self.seq_offsets)

    def _init_feat_info(self):
        """
        åˆå§‹åŒ–ç‰¹å¾ä¿¡æ¯, åŒ…æ‹¬ç‰¹å¾ç¼ºçœå€¼å’Œç‰¹å¾ç±»å‹

        Returns:
            feat_default_value: ç‰¹å¾ç¼ºçœå€¼ï¼Œæ¯ä¸ªå…ƒç´ ä¸ºå­—å…¸ï¼Œkeyä¸ºç‰¹å¾IDï¼Œvalueä¸ºç‰¹å¾ç¼ºçœå€¼
            feat_types: ç‰¹å¾ç±»å‹ï¼Œkeyä¸ºç‰¹å¾ç±»å‹åç§°ï¼Œvalueä¸ºåŒ…å«çš„ç‰¹å¾IDåˆ—è¡¨
        """
        feat_default_value = {}
        feat_statistics = {}
        feat_types = {}
        feat_types['user_sparse'] = ['103', '104', '105', '109']
        feat_types['item_sparse'] = [
            '100',
            '117',
            '111',
            '118',
            '101',
            '102',
            '119',
            '120',
            '114',
            '112',
            '121',
            '115',
            '122',
            '116',
        ]
        feat_types['item_array'] = []
        feat_types['user_array'] = ['106', '107', '108', '110']
        feat_types['item_emb'] = self.mm_emb_ids
        feat_types['user_continual'] = []
        feat_types['item_continual'] = []

        for feat_id in feat_types['user_sparse']:
            feat_default_value[feat_id] = 0
            feat_statistics[feat_id] = len(self.indexer['f'][feat_id])
        for feat_id in feat_types['item_sparse']:
            feat_default_value[feat_id] = 0
            feat_statistics[feat_id] = len(self.indexer['f'][feat_id])
        for feat_id in feat_types['item_array']:
            feat_default_value[feat_id] = [0]
            feat_statistics[feat_id] = len(self.indexer['f'][feat_id])
        for feat_id in feat_types['user_array']:
            feat_default_value[feat_id] = [0]
            feat_statistics[feat_id] = len(self.indexer['f'][feat_id])
        for feat_id in feat_types['user_continual']:
            feat_default_value[feat_id] = 0
        for feat_id in feat_types['item_continual']:
            feat_default_value[feat_id] = 0
        for feat_id in feat_types['item_emb']:
            feat_default_value[feat_id] = np.zeros(
                list(self.mm_emb_dict[feat_id].values())[0].shape[0], dtype=np.float32
            )

        return feat_default_value, feat_types, feat_statistics

    def fill_missing_feat(self, feat, item_id):
        """
        å¯¹äºåŸå§‹æ•°æ®ä¸­ç¼ºå¤±çš„ç‰¹å¾è¿›è¡Œå¡«å……ç¼ºçœå€¼

        Args:
            feat: ç‰¹å¾å­—å…¸
            item_id: ç‰©å“ID

        Returns:
            filled_feat: å¡«å……åçš„ç‰¹å¾å­—å…¸
        """
        if feat == None:
            feat = {}
        filled_feat = {}
        for k in feat.keys():
            filled_feat[k] = feat[k]

        all_feat_ids = []
        for feat_type in self.feature_types.values():
            all_feat_ids.extend(feat_type)
        # ç¼ºå¤±çš„fieldså¯èƒ½æ˜¯ sparse, array, continual, emb
        missing_fields = set(all_feat_ids) - set(feat.keys())
        # å¦‚æœä¸æ˜¯mmembï¼Œç”¨é»˜è®¤å€¼å¡«å……
        for feat_id in missing_fields:
            filled_feat[feat_id] = self.feature_default_value[feat_id]
        # å¦‚æœæ˜¯mmembï¼Œè¿™é‡Œåº”è¯¥èµ°çš„ä¸ç¼ºå¤±çš„é€»è¾‘ï¼Œæ˜¯æ‰¾åˆ°item_idå¯¹åº”çš„embå¡«ä¸Šå»ï¼›åŸæ¥æ²¡æœ‰åªæ˜¯å› ä¸ºseq.jsonlé‡Œé¢æ²¡æœ‰è¿™ä¸ªæ•°æ®
        for feat_id in self.feature_types['item_emb']:
            if item_id != 0 and self.indexer_i_rev[item_id] in self.mm_emb_dict[feat_id]:
                if type(self.mm_emb_dict[feat_id][self.indexer_i_rev[item_id]]) == np.ndarray:
                    filled_feat[feat_id] = self.mm_emb_dict[feat_id][self.indexer_i_rev[item_id]]

        return filled_feat

    @staticmethod
    def collate_fn(batch):
        """
        Args:
            batch: å¤šä¸ª__getitem__è¿”å›çš„æ•°æ®

        Returns:
            seq: ç”¨æˆ·åºåˆ—ID, torch.Tensorå½¢å¼
            pos: æ­£æ ·æœ¬ID, torch.Tensorå½¢å¼
            neg: è´Ÿæ ·æœ¬ID, torch.Tensorå½¢å¼
            token_type: ç”¨æˆ·åºåˆ—ç±»å‹, torch.Tensorå½¢å¼
            next_token_type: ä¸‹ä¸€ä¸ªtokenç±»å‹, torch.Tensorå½¢å¼
            seq_feat: ç”¨æˆ·åºåˆ—ç‰¹å¾, listå½¢å¼
            pos_feat: æ­£æ ·æœ¬ç‰¹å¾, listå½¢å¼
            neg_feat: è´Ÿæ ·æœ¬ç‰¹å¾, listå½¢å¼
        """
        seq, pos, neg, token_type, next_token_type, next_action_type, seq_feat, pos_feat, neg_feat = zip(*batch)
        seq = torch.from_numpy(np.array(seq))
        pos = torch.from_numpy(np.array(pos))
        neg = torch.from_numpy(np.array(neg))
        token_type = torch.from_numpy(np.array(token_type))
        next_token_type = torch.from_numpy(np.array(next_token_type))
        next_action_type = torch.from_numpy(np.array(next_action_type))
        seq_feat = list(seq_feat)
        pos_feat = list(pos_feat)
        neg_feat = list(neg_feat)
        return seq, pos, neg, token_type, next_token_type, next_action_type, seq_feat, pos_feat, neg_feat


class MyTestDataset(MyDataset):
    """
    æµ‹è¯•æ•°æ®é›†
    """

    def __init__(self, data_dir, args):
        super().__init__(data_dir, args)

    def _load_data_and_offsets(self):
        self.data_file = open(self.data_dir / "predict_seq.jsonl", 'rb')
        with open(Path(self.data_dir, 'predict_seq_offsets.pkl'), 'rb') as f:
            self.seq_offsets = pickle.load(f)

    def _process_cold_start_feat(self, feat):
        """
        å¤„ç†å†·å¯åŠ¨ç‰¹å¾ã€‚è®­ç»ƒé›†æœªå‡ºç°è¿‡çš„ç‰¹å¾valueä¸ºå­—ç¬¦ä¸²ï¼Œé»˜è®¤è½¬æ¢ä¸º0.å¯è®¾è®¡æ›¿æ¢ä¸ºæ›´å¥½çš„æ–¹æ³•ã€‚
        """
        processed_feat = {}
        for feat_id, feat_value in feat.items():
            if type(feat_value) == list:
                value_list = []
                for v in feat_value:
                    if type(v) == str:
                        value_list.append(0)
                    else:
                        value_list.append(v)
                processed_feat[feat_id] = value_list
            elif type(feat_value) == str:
                processed_feat[feat_id] = 0
            else:
                processed_feat[feat_id] = feat_value
        return processed_feat

    def __getitem__(self, uid):
        """
        è·å–å•ä¸ªç”¨æˆ·çš„æ•°æ®ï¼Œå¹¶è¿›è¡Œpaddingå¤„ç†ï¼Œç”Ÿæˆæ¨¡å‹éœ€è¦çš„æ•°æ®æ ¼å¼

        Args:
            uid: ç”¨æˆ·åœ¨self.data_fileä¸­å‚¨å­˜çš„è¡Œå·
        Returns:
            seq: ç”¨æˆ·åºåˆ—ID
            token_type: ç”¨æˆ·åºåˆ—ç±»å‹ï¼Œ1è¡¨ç¤ºitemï¼Œ2è¡¨ç¤ºuser
            seq_feat: ç”¨æˆ·åºåˆ—ç‰¹å¾ï¼Œæ¯ä¸ªå…ƒç´ ä¸ºå­—å…¸ï¼Œkeyä¸ºç‰¹å¾IDï¼Œvalueä¸ºç‰¹å¾å€¼
            user_id: user_id eg. user_xxxxxx ,ä¾¿äºåé¢å¯¹ç…§ç­”æ¡ˆ
        """
        user_sequence = self._load_user_data(uid)  # åŠ¨æ€åŠ è½½ç”¨æˆ·æ•°æ®

        ext_user_sequence = []
        for record_tuple in user_sequence:
            u, i, user_feat, item_feat, _, _ = record_tuple
            if u:
                if type(u) == str:  # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œè¯´æ˜æ˜¯user_id
                    user_id = u
                else:  # å¦‚æœæ˜¯intï¼Œè¯´æ˜æ˜¯re_id
                    user_id = self.indexer_u_rev[u]
            if u and user_feat:
                if type(u) == str:
                    u = 0
                if user_feat:
                    user_feat = self._process_cold_start_feat(user_feat)
                ext_user_sequence.insert(0, (u, user_feat, 2))

            if i and item_feat:
                # åºåˆ—å¯¹äºè®­ç»ƒæ—¶æ²¡è§è¿‡çš„itemï¼Œä¸ä¼šç›´æ¥èµ‹0ï¼Œè€Œæ˜¯ä¿ç•™creative_idï¼Œcreative_idè¿œå¤§äºè®­ç»ƒæ—¶çš„itemnum
                if i > self.itemnum:
                    i = 0
                if item_feat:
                    item_feat = self._process_cold_start_feat(item_feat)
                ext_user_sequence.append((i, item_feat, 1))

        seq = np.zeros([self.maxlen + 1], dtype=np.int32)
        token_type = np.zeros([self.maxlen + 1], dtype=np.int32)
        seq_feat = np.empty([self.maxlen + 1], dtype=object)

        idx = self.maxlen

        ts = set()
        for record_tuple in ext_user_sequence:
            if record_tuple[2] == 1 and record_tuple[0]:
                ts.add(record_tuple[0])

        for record_tuple in reversed(ext_user_sequence[:-1]):
            i, feat, type_ = record_tuple
            feat = self.fill_missing_feat(feat, i)
            seq[idx] = i
            token_type[idx] = type_
            seq_feat[idx] = feat
            idx -= 1
            if idx == -1:
                break

        seq_feat = np.where(seq_feat == None, self.feature_default_value, seq_feat)

        return seq, token_type, seq_feat, user_id

    def __len__(self):
        """
        Returns:
            len(self.seq_offsets): ç”¨æˆ·æ•°é‡
        """
        with open(Path(self.data_dir, 'predict_seq_offsets.pkl'), 'rb') as f:
            temp = pickle.load(f)
        return len(temp)

    @staticmethod
    def collate_fn(batch):
        """
        å°†å¤šä¸ª__getitem__è¿”å›çš„æ•°æ®æ‹¼æ¥æˆä¸€ä¸ªbatch

        Args:
            batch: å¤šä¸ª__getitem__è¿”å›çš„æ•°æ®

        Returns:
            seq: ç”¨æˆ·åºåˆ—ID, torch.Tensorå½¢å¼
            token_type: ç”¨æˆ·åºåˆ—ç±»å‹, torch.Tensorå½¢å¼
            seq_feat: ç”¨æˆ·åºåˆ—ç‰¹å¾, listå½¢å¼
            user_id: user_id, str
        """
        seq, token_type, seq_feat, user_id = zip(*batch)
        seq = torch.from_numpy(np.array(seq))
        token_type = torch.from_numpy(np.array(token_type))
        seq_feat = list(seq_feat)

        return seq, token_type, seq_feat, user_id


def save_emb(emb, save_path):
    """
    å°†Embeddingä¿å­˜ä¸ºäºŒè¿›åˆ¶æ–‡ä»¶

    Args:
        emb: è¦ä¿å­˜çš„Embeddingï¼Œå½¢çŠ¶ä¸º [num_points, num_dimensions]
        save_path: ä¿å­˜è·¯å¾„
    """
    num_points = emb.shape[0]  # æ•°æ®ç‚¹æ•°é‡
    num_dimensions = emb.shape[1]  # å‘é‡çš„ç»´åº¦
    print(f'saving {save_path}')
    with open(Path(save_path), 'wb') as f:
        f.write(struct.pack('II', num_points, num_dimensions))
        emb.tofile(f)


def load_mm_emb(mm_path, feat_ids):
    """
    åŠ è½½å¤šæ¨¡æ€ç‰¹å¾Embedding

    Args:
        mm_path: å¤šæ¨¡æ€ç‰¹å¾Embeddingè·¯å¾„
        feat_ids: è¦åŠ è½½çš„å¤šæ¨¡æ€ç‰¹å¾IDåˆ—è¡¨

    Returns:
        mm_emb_dict: å¤šæ¨¡æ€ç‰¹å¾Embeddingå­—å…¸ï¼Œkeyä¸ºç‰¹å¾IDï¼Œvalueä¸ºç‰¹å¾Embeddingå­—å…¸ï¼ˆkeyä¸ºitem IDï¼Œvalueä¸ºEmbeddingï¼‰
    """
    SHAPE_DICT = {"81": 32, "82": 1024, "83": 3584, "84": 4096, "85": 3584, "86": 3584}
    mm_emb_dict = {}
    for feat_id in tqdm(feat_ids, desc='Loading mm_emb'):
        shape = SHAPE_DICT[feat_id]
        emb_dict = {}
        if feat_id != '81':
            try:
                base_path = Path(mm_path, f'emb_{feat_id}_{shape}')
                # æ£€æŸ¥base_pathä¸‹æ˜¯å¦æœ‰ json æ–‡ä»¶
                json_files = list(base_path.glob('*.json'))
                if not json_files:
                    # å¦‚æœæ²¡æœ‰jsonæ–‡ä»¶ï¼Œåˆ™æŸ¥æ‰¾.partæ–‡ä»¶
                    json_files = list(base_path.glob('part*'))
                for json_file in json_files:
                    with open(json_file, 'r', encoding='utf-8') as file:
                        for line in file:
                            data_dict_origin = json.loads(line.strip())
                            insert_emb = data_dict_origin['emb']
                            if isinstance(insert_emb, list):
                                insert_emb = np.array(insert_emb, dtype=np.float32)
                            data_dict = {data_dict_origin['anonymous_cid']: insert_emb}
                            emb_dict.update(data_dict)
            except Exception as e:
                print(f"transfer error: {e}")
        if feat_id == '81':
            with open(Path(mm_path, f'emb_{feat_id}_{shape}.pkl'), 'rb') as f:
                emb_dict = pickle.load(f)
        mm_emb_dict[feat_id] = emb_dict
        print(f'Loaded #{feat_id} mm_emb')
    return mm_emb_dict


if __name__ == '__main__':
    import os
    from main import get_args
    # æ£€æµ‹ç¯å¢ƒå˜é‡æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™å°è¯•åŠ è½½
    required_env_vars = ['TRAIN_LOG_PATH', 'TRAIN_TF_EVENTS_PATH', 'TRAIN_DATA_PATH', 'TRAIN_CKPT_PATH']
    missing_vars = [var for var in required_env_vars if not os.environ.get(var)]
    if missing_vars:
        try:
            from load_env import load_env
            load_env()
            print("å·²ä» .env æ–‡ä»¶åŠ è½½ç¯å¢ƒå˜é‡")
        except ImportError:
            print('æœªæ‰¾åˆ° .env æ–‡ä»¶')
    else:
        print("ç¯å¢ƒå˜é‡å·²å­˜åœ¨ï¼Œè·³è¿‡åŠ è½½")

    def feat_stat():
        # global dataset
        data_path = os.environ.get('TRAIN_DATA_PATH')
        args = get_args()
        dataset = MyDataset(data_path, args)
        feature_default_value, feature_types, feat_statistics = dataset._init_feat_info()

        print("=" * 80)
        print("ç‰¹å¾ä¿¡æ¯æ±‡æ€»")
        print("=" * 80)

        # 1. æŒ‰ç±»å‹åˆ†ç»„æ˜¾ç¤ºç‰¹å¾ç»Ÿè®¡
        print("\nğŸ“Š ç‰¹å¾ç±»å‹åˆ†å¸ƒ:")
        print("-" * 50)
        for feat_type, feat_ids in feature_types.items():
            if feat_ids:  # åªæ˜¾ç¤ºéç©ºçš„ç‰¹å¾ç±»å‹
                print(f"{feat_type:15s}: {len(feat_ids):3d} ä¸ªç‰¹å¾ -> {feat_ids}")
            else:
                print(f"{feat_type:15s}: {len(feat_ids):3d} ä¸ªç‰¹å¾")

        # 2. æ˜¾ç¤ºç‰¹å¾é»˜è®¤å€¼ï¼ˆæŒ‰ç±»å‹åˆ†ç»„ï¼‰
        print(f"\nğŸ”§ ç‰¹å¾é»˜è®¤å€¼:")
        print("-" * 50)
        for feat_type, feat_ids in feature_types.items():
            if feat_ids:
                print(f"\n{feat_type}:")
                for feat_id in feat_ids:
                    default_val = feature_default_value[feat_id]
                    if isinstance(default_val, np.ndarray):
                        print(f"  {feat_id}: numpyæ•°ç»„(shape={default_val.shape}, dtype={default_val.dtype})")
                    elif isinstance(default_val, list):
                        print(f"  {feat_id}: {default_val}")
                    else:
                        print(f"  {feat_id}: {default_val}")

        # 3. æ˜¾ç¤ºç‰¹å¾ç»Ÿè®¡ä¿¡æ¯ï¼ˆæŒ‰ç±»å‹åˆ†ç»„ï¼Œå¹¶æ’åºï¼‰
        print(f"\nğŸ“ˆ ç‰¹å¾ç»Ÿè®¡ä¿¡æ¯:")
        print("-" * 50)
        for feat_type, feat_ids in feature_types.items():
            if feat_ids and feat_ids[0] in feat_statistics:  # æœ‰ç»Ÿè®¡ä¿¡æ¯çš„ç‰¹å¾ç±»å‹
                print(f"\n{feat_type}:")
                # æŒ‰ç»Ÿè®¡å€¼æ’åº
                sorted_feats = sorted(feat_ids, key=lambda x: feat_statistics.get(x, 0), reverse=True)
                for feat_id in sorted_feats:
                    if feat_id in feat_statistics:
                        count = feat_statistics[feat_id]
                        print(f"  {feat_id}: {count:8,} ä¸ªä¸åŒå€¼")

        # 4. æ€»ä½“ç»Ÿè®¡
        print(f"\nğŸ“‹ æ€»ä½“ç»Ÿè®¡:")
        print("-" * 50)
        total_features = sum(len(feat_ids) for feat_ids in feature_types.values())
        features_with_stats = len(feat_statistics)
        total_unique_values = sum(feat_statistics.values())

        print(f"æ€»ç‰¹å¾æ•°é‡: {total_features}")
        print(f"æœ‰ç»Ÿè®¡ä¿¡æ¯çš„ç‰¹å¾æ•°é‡: {features_with_stats}")
        print(f"æ‰€æœ‰ç‰¹å¾çš„å”¯ä¸€å€¼æ€»æ•°: {total_unique_values:,}")

        # 5. ç‰¹å¾è§„æ¨¡åˆ†æ
        if feat_statistics:
            print(f"\nğŸ“Š ç‰¹å¾è§„æ¨¡åˆ†æ:")
            print("-" * 50)
            stats_values = list(feat_statistics.values())
            print(f"æœ€å¤§ç‰¹å¾è§„æ¨¡: {max(stats_values):,}")
            print(f"æœ€å°ç‰¹å¾è§„æ¨¡: {min(stats_values):,}")
            print(f"å¹³å‡ç‰¹å¾è§„æ¨¡: {np.mean(stats_values):,.1f}")
            print(f"ä¸­ä½æ•°ç‰¹å¾è§„æ¨¡: {np.median(stats_values):,.1f}")

        print("=" * 80)
    feat_stat()

